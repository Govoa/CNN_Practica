{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as ker\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Rescaling\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No tocar\n",
    "num_clases = 26\n",
    "# La foto es 128x96\n",
    "xpixel = 96\n",
    "ypixel = 128\n",
    "# Tocar\n",
    "n_neuronas_conv1 = 64\n",
    "n_neuronas_conv2 = 128\n",
    "n_neuronas_conv3 = 256\n",
    "l_rate = 0.001  # empezar en 0.001 e ir bajando para el estudio\n",
    "epoch = 15\n",
    "batch = 1  # Realmente en 1 esta bien esto es mas para tiempos de ejecucion con grandes cantidades de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meter Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatos():\n",
    "    directorio = r'../Dataset/'\n",
    "    df = pd.read_csv(directorio + 'driver_imgs_list.csv')\n",
    "\n",
    "    fotos = df['img'].values\n",
    "    conductores = df['subject'].values\n",
    "    clases = df['classname'].values\n",
    "    # lo de rutas es magico la vd jejejejej\n",
    "    rutas = clases + '/' + fotos\n",
    "    dataset = pd.DataFrame([rutas , conductores])\n",
    "\n",
    "    imagenes = ImageDataGenerator(\n",
    "    validation_split=0.2,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range = [0.6, 1.4],\n",
    "    brightness_range = [0.3, 1.4]\n",
    "        ) #Aqui podemos hacer augmentation\n",
    "\n",
    "    \n",
    "    datasetEntero = dataset.T\n",
    "    datasetEntero.columns = ['fotos','conductores'] #que sino no  sabe que cojer xcol e ycol\n",
    " \n",
    "    directorio2 = r'../Dataset/imgs/train/'\n",
    "\n",
    "    entrenamiento = imagenes.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        subset=\"training\",\n",
    "        batch_size=batch,\n",
    "        seed=123,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel)\n",
    "    )\n",
    "    \n",
    "    validacion = imagenes.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        subset=\"validation\",\n",
    "        batch_size=batch,\n",
    "        seed=123,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel)\n",
    "    )\n",
    "    return entrenamiento , validacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range = [0.6, 1.4],\n",
    "    brightness_range = [0.3, 1.4]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelar Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_trampas(x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # drop 50% of neurons\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin pooling\n",
    "def modelar_red_2(x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(x, y, 3)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))  # drop 50% of neurons\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pooling\n",
    "def modelar_red_3(x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.2))  # drop 50% of neurons\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# obtencion de datos modo:trampa\n",
    "import cv2 \n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "nombres = []\n",
    "contador = 0\n",
    "\n",
    "for i in range (10):\n",
    "    contador = 0\n",
    "    print(i)\n",
    "    for filename in os.listdir('../Dataset/imgs/train/c'+str(i)+'/'):\n",
    "        if contador < 2000:\n",
    "            img = cv2.imread(os.path.join('../Dataset/imgs/train/c'+str(i)+'/',filename))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img,(128,96))\n",
    "            images.append(img/255)\n",
    "            nombres.append(filename)\n",
    "        contador += 1\n",
    "\n",
    "#---------------\n",
    "\n",
    "df_1= pd.read_csv(\"../Dataset/driver_imgs_list.csv\")\n",
    "df_2= pd.DataFrame()\n",
    "df_2['array'] = images\n",
    "df_2['img'] = nombres\n",
    "\n",
    "df_final = df_1.merge(df_2, on=\"img\", how=\"left\")\n",
    "\n",
    "df_final = df_final.dropna()\n",
    "\n",
    "#---------------\n",
    "\n",
    "imagenes = df_final['array'].tolist()\n",
    "personas = LabelBinarizer().fit_transform(df_final.subject)\n",
    "\n",
    "array_personas = df_final['subject'].to_numpy()\n",
    "array_personas = OrderedDict.fromkeys(array_personas)\n",
    "array_personas = list(filter(None, array_personas))\n",
    "\n",
    "#---------------\n",
    "\n",
    "entrenamiento_entradas,validacion_entradas,entrenamiento_salidas,validacion_salidas = train_test_split(imagenes, personas,test_size=0.2)\n",
    "\n",
    "entrenamiento_entradas = np.array(entrenamiento_entradas).astype(float)\n",
    "validacion_entradas = np.array(validacion_entradas).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pooling\n",
    "def modelar_red_4(x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "\n",
    "  # Capas convolucionales\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),strides=1,activation='relu',padding='same',input_shape=(x,y,3)))\n",
    "    model.add(MaxPooling2D((2,2),padding='same'))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),strides=1,activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2),padding='same'))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3),strides=1,activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D((2,2),padding='same'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Capas del MLP\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch, logs={}):\n",
    "        if(logs.get('loss') <= 0.2):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def Entrenar(m,e,v ,epo, b):\n",
    "    # Para parar segun el criterio de la pr\n",
    "    elcallback = myCallback()\n",
    "    \n",
    "    history = m.fit(\n",
    "        x = e,\n",
    "        validation_data = v,\n",
    "        epochs=epo,\n",
    "        batch_size=b,\n",
    "        verbose=1,  # Esto te imprime un progress bar con informacion\n",
    "        shuffle=True,\n",
    "        callbacks= elcallback\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluar(m, e_test):\n",
    "    return m.evaluate(e_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Esteticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos nuestro modelo\n",
    "\n",
    "def pintarmodelo(modelo):\n",
    "    tf.keras.utils.plot_model(\n",
    "        modelo,\n",
    "        to_file='model.png',\n",
    "        show_shapes=False,\n",
    "        show_dtype=False,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96,\n",
    "        layer_range=None,\n",
    "        show_layer_activations=False\n",
    "    )\n",
    "\n",
    "# Pintamos imagenes\n",
    "def mostrarimagenes(imagenes, batch):\n",
    "    img, labels = next(imagenes)\n",
    "    for i in range(batch):\n",
    "        plt.imshow(img[i, :, :, :])\n",
    "        plt.title(labels[i])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar y cargar el modelo neuronal deseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarmodelo():\n",
    "    model = ker.models.load_model('path/to/location')  # Carga modelos\n",
    "    return model\n",
    "    \n",
    "def guardarmodelo():\n",
    "    model.save('ModeloGuardado')  # guarda el modelo en la ruta que desees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17940 validated image filenames belonging to 26 classes.\n",
      "Found 4484 validated image filenames belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train, validation = meterdatos()\n",
    "# Puedes cambiar la funcion para elegir otro modelo\n",
    "# model = Modelar_red_trampas(xpixel, ypixel, l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nhistory = Entrenar(model, train, validation, epoch, batch)\\n\\nplt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\\nplt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label = 'val_loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# si no satisface al callback esto parara\n",
    "''' \n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_loss, train_acc = Evaluar(model,train)\\nval_loss, val_acc = Evaluar(model,validation)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de diferentes modelos de red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = modelar_red_2(xpixel, ypixel, l_rate)\\nhistory = Entrenar(model, train, validation, epoch, batch)\\n\\nplt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\\nplt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label = 'val_loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = modelar_red_2(xpixel, ypixel, l_rate)\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = modelar_red_3(xpixel, ypixel, l_rate)\\nhistory = Entrenar(model, train, validation, epoch, batch)\\n\\nplt.plot(history.history['accuracy'], label='accuracy')\\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\\nplt.plot(history.history['loss'], label='loss')\\nplt.plot(history.history['val_loss'], label = 'val_loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.legend(loc='lower right')\\n\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = modelar_red_3(xpixel, ypixel, l_rate)\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 2874/17940 [===>..........................] - ETA: 2:10 - loss: 1.1437 - accuracy: 0.6552"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21124/3661970308.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelar_red_4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxpixel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypixel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntrenar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelar_red_4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxpixel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypixel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Nueva red\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m '''\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21124/2975301176.py\u001b[0m in \u001b[0;36mEntrenar\u001b[1;34m(m, e, v, epo, b)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0melcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     history = m.fit(\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "model = modelar_red_4(xpixel, ypixel, l_rate)\n",
    "history = Entrenar(model,train,validation, epoch, batch)\n",
    "model = modelar_red_4(xpixel,ypixel,l_rate)\n",
    "# Nueva red\n",
    "'''\n",
    "model = modelar_red_4(xpixel,ypixel,l_rate)\n",
    "history = model.fit(entrenamiento_entradas, \n",
    "entrenamiento_salidas,epochs=15,verbose=1,\n",
    "validation_data=(validacion_entradas, validacion_salidas), callbacks=myCallback())\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e07a3e522892b4e580891ac3cbd59d54ad55eda1c58d964756061ebcf1ca01b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
