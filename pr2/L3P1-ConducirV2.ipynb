{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import tensorflow as tf\n",
    "import keras as ker\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Rescaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables a definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# No tocar\n",
    "num_clases = 26\n",
    "# La foto es 128x96\n",
    "xpixel = 128\n",
    "ypixel = 96\n",
    "# Tocar\n",
    "n_neuronas_conv1 = 64\n",
    "n_neuronas_conv2 = 128\n",
    "n_neuronas_conv3 = 256\n",
    "l_rate = 0.001  # empezar en 0.001 e ir bajando para el estudio\n",
    "epoch = 5\n",
    "batch = 16  # Realmente en 1 esta bien esto es mas para tiempos de ejecucion con grandes cantidades de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los dataset\n",
    "\n",
    "Explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatos():\n",
    "    directorio = r'../Dataset/'\n",
    "    df = pd.read_csv(directorio + 'driver_imgs_list.csv')\n",
    "    # Mezclamos los datos muy importante\n",
    "    df = df.sample(frac=1)\n",
    "    fotos = df['img'].values\n",
    "    conductores = df['subject'].values\n",
    "    clases = df['classname'].values\n",
    "    # lo de rutas es magico la vd jejejejej\n",
    "    rutas = clases + '/' + fotos\n",
    "    dataset = pd.DataFrame([rutas , conductores])\n",
    "\n",
    "    imagenesv = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2,\n",
    "        ) #Aqui podemos hacer augmentation\n",
    "\n",
    "    \n",
    "    datasetEntero = dataset.T\n",
    "    datasetEntero.columns = ['fotos','conductores'] #que sino no  sabe que cojer xcol e ycol\n",
    " \n",
    "    directorio2 = r'../Dataset/imgs/train/'\n",
    "\n",
    "    entrenamiento = imagenesv.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        batch_size=batch,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel),\n",
    "        subset='training'\n",
    "        \n",
    "    )\n",
    "    validacion = imagenesv.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        batch_size=batch,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel),\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    return entrenamiento,validacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatosaug():\n",
    "    directorio = r'../Dataset/'\n",
    "    df = pd.read_csv(directorio + 'driver_imgs_list.csv')\n",
    "    # Mezclamos los datos muy importante\n",
    "    df = df.sample(frac=1)\n",
    "    fotos = df['img'].values\n",
    "    conductores = df['subject'].values\n",
    "    clases = df['classname'].values\n",
    "    # lo de rutas es magico la vd jejejejej\n",
    "    rutas = clases + '/' + fotos\n",
    "    dataset = pd.DataFrame([rutas , conductores])\n",
    "\n",
    "    imagenesv = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=30, \n",
    "        zoom_range= [0.2 , 0.4],\n",
    "        fill_mode='nearest',\n",
    "        brightness_range = [0.2, 0.6]\n",
    "        \n",
    "        ) #Aqui podemos hacer augmentation\n",
    "\n",
    "    \n",
    "    datasetEntero = dataset.T\n",
    "    datasetEntero.columns = ['fotos','conductores'] #que sino no  sabe que cojer xcol e ycol\n",
    " \n",
    "    directorio2 = r'../Dataset/imgs/train/'\n",
    "\n",
    "    entrenamiento = imagenesv.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        batch_size=batch,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel),\n",
    "        subset='training'\n",
    "        \n",
    "    )\n",
    "    validacion = imagenesv.flow_from_dataframe(\n",
    "        dataframe=datasetEntero,\n",
    "        directory=directorio2,\n",
    "        x_col=\"fotos\",\n",
    "        y_col=\"conductores\",\n",
    "        batch_size=batch,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(xpixel,ypixel),\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    return entrenamiento,validacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la Red\n",
    "\n",
    "La siguiente red es la red que hemos considerado en la memoria que era la más óptima y la que hemos usado para el entrenamiento y la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red(n_conv1, n_conv2, n_conv3, x, y, lr): \n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(1024/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red\n",
    "\n",
    "Con la red ya creada, es el momento de realizar el entrenamiento. Primero de todo realizamos un callback para parar pronto si no cumple con el objetivo de un los mínimo de 0.2 (puesto que así esta matizado en el enunciado de la práctica). Es decir, se deja de entrenar cuando una métrica monitoreada haya dejado de mejorar. En este caso es el loss con el mínimo de 0.2.\n",
    "\n",
    "Respecto a las entradas de la función son las siguientes: (modelo de red, datos entrenamiento, datos validacion, epoch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss') <= 0.2):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def Entrenar(m, e, v, epo, b):\n",
    "    # Para parar segun el criterio de la pr\n",
    "    elcallback = myCallback()\n",
    "    \n",
    "    history = m.fit(\n",
    "        e,\n",
    "        validation_data=v,\n",
    "        epochs=epo,\n",
    "        batch_size=b,\n",
    "        verbose=1,  # Esto te imprime un progress bar con informacion\n",
    "        shuffle=True,\n",
    "        callbacks= elcallback\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la red\n",
    "\n",
    "El último paso entonces es la evaluación de la red, lo que nos permitirá saber si esta ha sido correctamente diseñada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluar(m, e_test):\n",
    "    return m.evaluate(e_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación para entrenar\n",
    "\n",
    "En la siguiente celda, ya con todas las funciones creadas procedemos a la ejecución de la creación de los set de datos y de la creación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17940 validated image filenames belonging to 26 classes.\n",
      "Found 4484 validated image filenames belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train, validation = meterdatos()\n",
    "# Puedes cambiar la funcion para elegir otro modelo\n",
    "model = Modelar_red(16,32,64,xpixel, ypixel, l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución\n",
    "\n",
    "Una vez tenemos tanto la red como los datos, la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1122/1122 [==============================] - 88s 77ms/step - loss: 0.0847 - accuracy: 0.9776 - val_loss: 1.6476e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f6cf99bf40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "\n",
    "Evaluamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122/1122 [==============================] - 69s 61ms/step - loss: 3.1175e-04 - accuracy: 0.9999\n",
      "281/281 [==============================] - 17s 60ms/step - loss: 1.6476e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "A continuación predecimos 10 imagenes de test, nunca usadas y de cada clase y vemos si son válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17772/662872977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'resultados_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[0mpintar_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[0mcsv_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17772/662872977.py\u001b[0m in \u001b[0;36mpintar_test\u001b[1;34m(red, v)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mprediccion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicciones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mtexto_prediccion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuscar_prediccion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediccion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17772/662872977.py\u001b[0m in \u001b[0;36mbuscar_prediccion\u001b[1;34m(prediccion)\u001b[0m\n\u001b[0;32m     37\u001b[0m     }\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mprimero\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediccion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediccion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mpos1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediccion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprimero\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mprediccion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprimero\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3960x3960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Como bien hemos comentado, en este caso haremos dos partes procesando 26 imagenes\n",
    "    - Las tres categorías en las que con mayor probabilidad la imagen es clasificada por la red, en orden \n",
    "    descendente de probabilidad \n",
    "    - La imagen pintada\n",
    "Le damos pues 26 imágenes, una de cada tipo \n",
    "\"\"\"\n",
    "def buscar_prediccion(prediccion): # Saber que predicción es\n",
    "    texto_prediccion = \"\"\n",
    "    diccionario = {\n",
    "        0: \"p002\",\n",
    "        1: \"p012\",\n",
    "        2: \"p014\",\n",
    "        3: \"p015\",\n",
    "        4: \"p016\",\n",
    "        5: \"p021\",\n",
    "        6: \"p022\",\n",
    "        7: \"p024\",\n",
    "        8: \"p026\",\n",
    "        9: \"p035\",\n",
    "        10: \"p039\",\n",
    "        11: \"p041\",\n",
    "        12: \"p042\",\n",
    "        13: \"p045\",\n",
    "        14: \"p047\",\n",
    "        15: \"p049\",\n",
    "        16: \"p050\",\n",
    "        17: \"p051\",\n",
    "        18: \"p052\",\n",
    "        19: \"p056\",\n",
    "        20: \"p061\",\n",
    "        21: \"p064\",\n",
    "        22: \"p066\",\n",
    "        23: \"p072\",\n",
    "        24: \"p075\",\n",
    "        25: \"p081\"\n",
    "    }\n",
    "\n",
    "    primero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos1 = prediccion[primero]\n",
    "    prediccion[primero] = -1\n",
    "    texto_prediccion1 = str(diccionario[primero[0][0]]) + \" \" + str(pos1[0]) + \"% // \"\n",
    "\n",
    "    segundo = np.where(prediccion == np.amax(prediccion))\n",
    "    pos2 = prediccion[segundo]\n",
    "    prediccion[segundo] = -1\n",
    "    texto_prediccion2 = str(diccionario[segundo[0][0]]) + \" \" + str(pos2[0]) + \"% // \"\n",
    "\n",
    "    tercero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos3 = prediccion[tercero]\n",
    "    texto_prediccion3 = str(diccionario[tercero[0][0]]) + \" \" + str(pos3[0]) + \"%\"\n",
    "\n",
    "    texto_prediccion = texto_prediccion1 + texto_prediccion2 + texto_prediccion3\n",
    "\n",
    "    return texto_prediccion\n",
    "\n",
    "def pintar_test(red, v): # Esto nos pinta las predicciones\n",
    "    rows = 20\n",
    "    columns =1\n",
    "\n",
    "    predicciones = red.predict(v)\n",
    "    predicciones *= 100\n",
    "    \n",
    "    fig = plt.figure(figsize=(55, 55))\n",
    "\n",
    "    for i in range(20):\n",
    "        prediccion = predicciones[i]\n",
    "        texto_prediccion = buscar_prediccion(prediccion)\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(v[i][0][0])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(texto_prediccion)\n",
    "            \n",
    "\n",
    "# Creación del csv. Como son 3.4GB de fotos de test no se adjunta las imágenes en el entregable pero\n",
    "# este es el código que hemos usado para crear el csv\n",
    "\n",
    "def csv_test(red, v): \n",
    "    \n",
    "    predicciones = red.predict(v,verbose = 1)\n",
    "    predicciones = predicciones * 100\n",
    "    \n",
    "    # Añadimos las predicciones\n",
    "    df = pd.DataFrame(predicciones, columns = [\"p002\",\"p012\",\"p014\",\"p015\",\"p016\",\n",
    "    \"p021\",\"p022\",\"p024\",\"p026\",\"p035\",\"p039\",\"p041\",\"p042\",\"p045\",\"p047\",\"p049\",\n",
    "    \"p050\",\"p051\",\"p052\",\"p056\",\"p061\",\"p064\",\"p066\",\"p072\",\"p075\",\"p081\"])\n",
    "\n",
    "    df.to_csv('resultados_test.csv')\n",
    "\n",
    "pintar_test(model, validation)\n",
    "csv_test(model, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de otras arquitecturas de red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de convolución y pooling\n",
    "\n",
    "A continuación estudiaremos 4 arquitecturas de red sin dropout para ver cuales es la mejor arquitectura respecto a número de capas de convolución y pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de influencia de MLP\n",
    "\n",
    "A continuación estudiaremos 2 arquitecturas de red para ver cuales es la mejor arquitectura respecto al número de neuronas de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_5(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(32/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_6(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(1024/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_7(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(4096/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de dropout\n",
    "\n",
    "A continuación estudiaremos las 4 primeras arquitecturas de red con dropout para ver cuales es la influencia del dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DPlus(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.6))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DExce(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu' , input_shape=(x, y, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.9))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de diferentes redes\n",
    "\n",
    "Una vez tenemos todas las posibles redes, las entrenamos y evaluamos para ver su output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio arquitecturas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 1\n",
    "\n",
    "3 capas de convolución con su respectivo pooling cada una. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 2\n",
    "\n",
    "3 capas de convolución seguidas con un único pooling al final. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 3\n",
    "\n",
    "3 capas de convolución, con un pooling en la 1º y 3º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4\n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DPlus(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DExce(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 5\n",
    "\n",
    "2^5 neuronal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_5(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 6\n",
    "\n",
    "2^10 neuronas MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_6(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio learning rates\n",
    "\n",
    "Por último estudiamos el learning rate más adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rates = [0.00001,0.0001,0.001,0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[0])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[0])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[1])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[1])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[2])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[2])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[3])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[3])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e07a3e522892b4e580891ac3cbd59d54ad55eda1c58d964756061ebcf1ca01b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
