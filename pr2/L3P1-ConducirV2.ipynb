{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables a definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los dataset\n",
    "\n",
    "Explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatos():\n",
    "    print(\"aqui\")\n",
    "    # return entrenamiento , validacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la Red\n",
    "\n",
    "La siguiente red es la red que hemos considerado en la memoria que era la más óptima y la que hemos usado para el entrenamiento y la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red(x, y, lr): \n",
    "    model = Sequential()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red\n",
    "\n",
    "Con la red ya creada, es el momento de realizar el entrenamiento. Primero de todo realizamos un callback para parar pronto si no cumple con el objetivo de un los mínimo de 0.2 (puesto que así esta matizado en el enunciado de la práctica). Es decir, se deja de entrenar cuando una métrica monitoreada haya dejado de mejorar. En este caso es el loss con el mínimo de 0.2.\n",
    "\n",
    "Respecto a las entradas de la función son las siguientes: (modelo de red, datos entrenamiento, datos validacion, epoch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss') <= 0.2):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def Entrenar(m, e, v, epo, b):\n",
    "    # Para parar segun el criterio de la pr\n",
    "    elcallback = myCallback()\n",
    "    \n",
    "    history = m.fit(\n",
    "        e,\n",
    "        validation_data=v,\n",
    "        epochs=epo,\n",
    "        batch_size=b,\n",
    "        verbose=1,  # Esto te imprime un progress bar con informacion\n",
    "        shuffle=True,\n",
    "        callbacks= elcallback\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la red\n",
    "\n",
    "El último paso entonces es la evaluación de la red, lo que nos permitirá saber si esta ha sido correctamente diseñada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluar(m, e_test):\n",
    "    return m.evaluate(e_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación para entrenar\n",
    "\n",
    "En la siguiente celda, ya con todas las funciones creadas procedemos a la ejecución de la creación de los set de datos y de la creación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = meterdatos()\n",
    "# Puedes cambiar la funcion para elegir otro modelo\n",
    "model = Modelar_red(xpixel, ypixel, l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución\n",
    "\n",
    "Una vez tenemos tanto la red como los datos, la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "\n",
    "Evaluamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "A continuación predecimos 10 imagenes de test, nunca usadas y de cada clase y vemos si son válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Como bien hemos comentado, en este caso haremos dos partes procesando 26 imagenes\n",
    "    - Las tres categorías en las que con mayor probabilidad la imagen es clasificada por la red, en orden \n",
    "    descendente de probabilidad \n",
    "    - La imagen pintada\n",
    "Le damos pues 26 imágenes, una de cada tipo \n",
    "\"\"\"\n",
    "def buscar_prediccion(prediccion): # Saber que predicción es\n",
    "    texto_prediccion = \"\"\n",
    "    diccionario = {\n",
    "        0: \"\",\n",
    "        1: \"\",\n",
    "        2: \"\",\n",
    "        3: \"\",\n",
    "        4: \"\",\n",
    "        5: \"\",\n",
    "        6: \"\",\n",
    "        7: \"\",\n",
    "        8: \"\",\n",
    "        9: \"\",\n",
    "        10: \"\",\n",
    "        11: \"\",\n",
    "        12: \"\",\n",
    "        13: \"\",\n",
    "        14: \"\",\n",
    "        15: \"\",\n",
    "        16: \"\",\n",
    "        17: \"\",\n",
    "        18: \"\",\n",
    "        19: \"\",\n",
    "        20: \"\",\n",
    "        21: \"\",\n",
    "        22: \"\",\n",
    "        23: \"\",\n",
    "        24: \"\",\n",
    "        25: \"\"\n",
    "    }\n",
    "\n",
    "    primero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos1 = prediccion[primero]\n",
    "    prediccion[primero] = -1\n",
    "    texto_prediccion1 = str(diccionario[primero[0][0]]) + \" \" + str(pos1[0]) + \"% // \"\n",
    "\n",
    "    segundo = np.where(prediccion == np.amax(prediccion))\n",
    "    pos2 = prediccion[segundo]\n",
    "    prediccion[segundo] = -1\n",
    "    texto_prediccion2 = str(diccionario[segundo[0][0]]) + \" \" + str(pos2[0]) + \"% // \"\n",
    "\n",
    "    tercero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos3 = prediccion[tercero]\n",
    "    texto_prediccion3 = str(diccionario[tercero[0][0]]) + \" \" + str(pos3[0]) + \"%\"\n",
    "\n",
    "    texto_prediccion = texto_prediccion1 + texto_prediccion2 + texto_prediccion3\n",
    "\n",
    "    return texto_prediccion\n",
    "\n",
    "def pintar_test(red): # Esto nos pinta las predicciones\n",
    "    rows = 14\n",
    "    columns =1\n",
    "\n",
    "    image_size = (xpixel, ypixel)\n",
    "\n",
    "    # Generamos el dataset de test (14 imagenes)\n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../Dataset/imgs/imgstest/',\n",
    "        image_size=image_size,\n",
    "        label_mode=None,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    predicciones = red.predict(test_dataset)\n",
    "    predicciones *= 100\n",
    "    \n",
    "    fig = plt.figure(figsize=(55, 55))\n",
    "\n",
    "    for images in test_dataset.take(1):\n",
    "        for i in range(9):\n",
    "            prediccion = predicciones[i]\n",
    "            texto_prediccion = buscar_prediccion(prediccion)\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(texto_prediccion)\n",
    "\n",
    "# Creación del csv. Como son 3.4GB de fotos de test no se adjunta las imágenes en el entregable pero\n",
    "# este es el código que hemos usado para crear el csv\n",
    "\n",
    "def csv_test(red): \n",
    "    image_size = (xpixel, ypixel)\n",
    "    \n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../../imgstest/',\n",
    "        image_size=image_size,\n",
    "        label_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    predicciones = red.predict(test_dataset,verbose = 1)\n",
    "    predicciones = predicciones * 100\n",
    "    \n",
    "    # Añadimos las predicciones\n",
    "    df = pd.DataFrame(predicciones, columns = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9'])\n",
    "    df.to_csv('resultados_test.csv')\n",
    "\n",
    "    # Añadimos el tag de el nombre de la imágen\n",
    "    nombre = os.listdir('../../imgstest/test')\n",
    "    df['imagen'] = nombre\n",
    "\n",
    "pintar_test(model)\n",
    "csv_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de otras arquitecturas de red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de convolución y pooling\n",
    "\n",
    "A continuación estudiaremos 4 arquitecturas de red sin dropout para ver cuales es la mejor arquitectura respecto a número de capas de convolución y pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de influencia de MLP\n",
    "\n",
    "A continuación estudiaremos 2 arquitecturas de red para ver cuales es la mejor arquitectura respecto al número de neuronas de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_5(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(32/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_6(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(1024/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_7(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(4096/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estudio de dropout\n",
    "\n",
    "A continuación estudiaremos las 4 primeras arquitecturas de red con dropout para ver cuales es la influencia del dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DPlus(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.6))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DExce(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.9))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de diferentes redes\n",
    "\n",
    "Una vez tenemos todas las posibles redes, las entrenamos y evaluamos para ver su output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio arquitecturas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 1\n",
    "\n",
    "3 capas de convolución con su respectivo pooling cada una. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 2\n",
    "\n",
    "3 capas de convolución seguidas con un único pooling al final. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 3\n",
    "\n",
    "3 capas de convolución, con un pooling en la 1º y 3º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4\n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Modelar_red_4_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DPlus(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con dropout 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DExce(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 5\n",
    "\n",
    "2^5 neuronal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_5(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 6\n",
    "\n",
    "2^10 neuronas MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_6(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 7\n",
    "\n",
    "2^12 neuronas MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_7(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio learning rates\n",
    "\n",
    "Por último estudiamos el learning rate más adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rates = [0.00001,0.0001,0.001,0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[0])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[0])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[1])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[1])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[2])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[2])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[3])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[3])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e07a3e522892b4e580891ac3cbd59d54ad55eda1c58d964756061ebcf1ca01b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
