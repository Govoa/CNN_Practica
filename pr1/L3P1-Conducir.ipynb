{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as ker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Rescaling # Descomentar para ejecutar con GPU \n",
    "# from keras.layers.experimental.preprocessing import Rescaling # Descomentar para ejecutar con CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables a definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No tocar\n",
    "num_clases = 10\n",
    "# La foto es 128x96\n",
    "xpixel = 128\n",
    "ypixel = 96\n",
    "# Tocar\n",
    "n_neuronas_conv1 = 64\n",
    "n_neuronas_conv2 = 128\n",
    "n_neuronas_conv3 = 256\n",
    "l_rate = 0.0001  # empezar en 0.001 e ir bajando para el estudio\n",
    "epoch = 15\n",
    "batch = 16  # Realmente en 1 esta bien esto es mas para tiempos de ejecucion con grandes cantidades de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los dataset\n",
    "\n",
    "Para el correcto entreno de la red neuronal es necesario crear dos dataset, uno de entreno y uno de validación. Este proceso lo hacemos gracias a la función **tf.keras.preprocessing.image_dataset_from_directory** que nos permite dado un directorio, recorrerlo iterativamente y crear una \"lista de tuplas de la siguiente forma\":\n",
    "\n",
    "- Primero una imagen convertida en array numérico de tres dimensiones (RGB)\n",
    "- A esta imagen esta asginado su clase. Esta clase la función la asigna directamente según en el subdirectorio en el que este localizada. Como podemos ver, nuestro directorio de imágenes tiene subdirectorios según la clase, de ahí coge el label creando así la tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatos(batch):\n",
    "\n",
    "    image_size = (xpixel, ypixel)\n",
    "    batch_size = batch\n",
    "\n",
    "    # Primero creamos el test con el 80% de todas las imagenes que se nos dan\n",
    "    train_generator_t = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../Dataset/imgs/train/',\n",
    "        label_mode='categorical',\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    # Segundo creamos el validation con el 20% de todas las imagenes que se nos dan\n",
    "    train_generator_v = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../Dataset/imgs/train/',\n",
    "        label_mode='categorical',\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        shuffle=True,\n",
    "        seed=1337,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Matizar un punto importante y es que ambos tienen el \"batchsize\". El batch size se define como\n",
    "    el número de muestras que se propagarán a través de la red. De esta manera, a la red no solo le pasamos\n",
    "    el 80%/20% de las muestras, si no que ese 80%/20% a su vez es dividido entre el batch size.\n",
    "\n",
    "    Por ejemplo, supongamos que tenemos 1050 muestras de entrenamiento y queremos configurar un \n",
    "    tamaño de lote igual a 100. El algoritmo toma las primeras 100 muestras (del 1 al 100) del \n",
    "    conjunto de datos de entrenamiento y entrena la red. A continuación, toma las segundas 100 muestras \n",
    "    (del 101 al 200) y vuelve a entrenar la red.\n",
    "    \n",
    "    Por tanto, siempre entrenamos con todas las imágenes (el 80% acaba siendo procesado entero por la red)\n",
    "    pero en cada iteración solo procesa el batch size de imágenes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return train_generator_t, train_generator_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los dataset con Augmentation\n",
    "\n",
    "Esta función es exactamente igual que la anterior pero tiene un valor añadido extremadamente alto. Al igual que la función anterior dividimos los dataset y automáticamente los procesamos mediante el recorrido iterativo de sus directorios. Pero en este caso tanto el entreno como la validación sufren cambios. Estos cambios son mediante Augmentation que consiste en crear sintéticamente nuevos datos de entrenamiento aplicando algunas transformaciones en los datos de entrada. \n",
    "\n",
    "De esta manera a cada imagen le pasamos unos \"filtros\" que la distorsionan, de tal manera que entrenamos la red más a fondo puesto que no aprende a reconocer a una imagen con X características si no que amplia mucho su visión. En la siguiente imagen podemos ver un ejemplo de Augmentation:\n",
    "\n",
    "![Augmentation](https://i.ibb.co/k2ZXKrR/Captura-de-pantalla-de-2022-03-30-09-54-39.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meterdatosaug():\n",
    "\n",
    "    train_datagen = ImageDataGenerator(  # Aqui se hacen los cambios a las imagenes\n",
    "        rescale=1./255, # Normalizar\n",
    "        shear_range=0.2, # Mover la imágen x pixeles\n",
    "        zoom_range=0.2, # Zoom\n",
    "        horizontal_flip=True, # 180º flip\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255) # Simplemente normalizar\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=r'../Dataset/imgs/train/',\n",
    "        target_size=(xpixel, ypixel),\n",
    "        batch_size=batch,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        directory=r'../Dataset/imgs/train/',\n",
    "        target_size=(xpixel, ypixel),\n",
    "        batch_size=batch,\n",
    "        class_mode='categorical')\n",
    "        \n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la red\n",
    "\n",
    "La siguiente red es la red que hemos considerado en la memoria que era la más óptima y la que hemos usado para el entrenamiento y la clasificación.\n",
    "\n",
    "Una red con tres capas de convolución, con un pooling en la 1º y 3º capa de convolución y dropout de 0.2. Por último, un MPL de 1024 y 512 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(1024/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red\n",
    "\n",
    "Con la red ya creada, es el momento de realizar el entrenamiento. Primero de todo realizamos un callback para parar pronto si no cumple con el objetivo de un los mínimo de 0.2 (puesto que así esta matizado en el enunciado de la práctica). Es decir, se deja de entrenar cuando una métrica monitoreada haya dejado de mejorar. En este caso es el loss con el mínimo de 0.2.\n",
    "\n",
    "Respecto a las entradas de la función son las siguientes: (modelo de red, datos entrenamiento, datos validacion, epoch, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss') <= 0.2):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def Entrenar(m, e, v, epo, b):\n",
    "    # Para parar segun el criterio de la pr\n",
    "    elcallback = myCallback()\n",
    "    \n",
    "    history = m.fit(\n",
    "        e,\n",
    "        validation_data=v,\n",
    "        epochs=epo,\n",
    "        batch_size=b,\n",
    "        verbose=1,  # Esto te imprime un progress bar con informacion\n",
    "        shuffle=True,\n",
    "        callbacks= elcallback\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de la red\n",
    "\n",
    "El último paso entonces es la evaluación de la red, lo que nos permitirá saber si esta ha sido correctamente diseñada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluar(m, e_test):\n",
    "    return m.evaluate(e_test,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones estéticas\n",
    "\n",
    "Estas funciones no tienen ningún fin concreto más allá de la estética. En estas se pinta el modelo neuronal. Estas funciones son principalmente usadas para la elaboración de la memoria de laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos nuestro modelo\n",
    "\n",
    "def pintarmodelo(modelo):\n",
    "    tf.keras.utils.plot_model(\n",
    "        modelo,\n",
    "        to_file='model.png',\n",
    "        show_shapes=False,\n",
    "        show_dtype=False,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96\n",
    "    )\n",
    "\n",
    "# mod = Modelar_red(32,64,128,xpixel, ypixel, l_rate)\n",
    "# pintarmodelo(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar y cargar el modelo neuronal deseado\n",
    "\n",
    "Para una ejecución más rápida y un ahorro de espacio, se puede guardar el modelo neuronal en una localización deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarmodelo():\n",
    "    model = ker.models.load_model('path/to/location')  # Carga modelos\n",
    "    return model\n",
    "    \n",
    "def guardarmodelo():\n",
    "    model.save('ModeloGuardado')  # guarda el modelo en la ruta que desees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación para entrenar\n",
    "\n",
    "En la siguiente celda, ya con todas las funciones creadas procedemos a la ejecución de la creación de los set de datos y de la creación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22424 files belonging to 10 classes.\n",
      "Using 17940 files for training.\n",
      "Found 22424 files belonging to 10 classes.\n",
      "Using 4484 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train, validation = meterdatos(batch)\n",
    "# Puedes cambiar la funcion para elegir otro modelo\n",
    "model = Modelar_red(32,64,128,xpixel, ypixel, l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución\n",
    "\n",
    "Una vez tenemos tanto la red como los datos, la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación\n",
    "\n",
    "Evaluamos nuestro modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "A continuación predecimos 10 imagenes de test, nunca usadas y de cada clase y vemos si son válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Como bien hemos comentado, en este caso haremos dos partes procesando 9 imagenes\n",
    "    - Las tres categorías en las que con mayor probabilidad la imagen es clasificada por la red, en orden \n",
    "    descendente de probabilidad \n",
    "    - La imagen pintada\n",
    "Le damos pues 10 imágenes, una de cada tipo \n",
    "\"\"\"\n",
    "def buscar_prediccion(prediccion): # Saber que predicción es\n",
    "    texto_prediccion = \"\"\n",
    "    diccionario = {\n",
    "        0: \"safe driving\",\n",
    "        1: \"texting - right\",\n",
    "        2: \"talking on the phone - right\",\n",
    "        3: \"texting - left\",\n",
    "        4: \"talking on the phone - left\",\n",
    "        5: \"operating the radio\",\n",
    "        6: \"drinking\",\n",
    "        7: \"reaching behind\",\n",
    "        8: \"hair and makeup\",\n",
    "        9: \"talking to passenger\",\n",
    "    }\n",
    "\n",
    "    primero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos1 = prediccion[primero]\n",
    "    prediccion[primero] = -1\n",
    "    texto_prediccion1 = str(diccionario[primero[0][0]]) + \" \" + str(pos1[0]) + \"% // \"\n",
    "\n",
    "    segundo = np.where(prediccion == np.amax(prediccion))\n",
    "    pos2 = prediccion[segundo]\n",
    "    prediccion[segundo] = -1\n",
    "    texto_prediccion2 = str(diccionario[segundo[0][0]]) + \" \" + str(pos2[0]) + \"% // \"\n",
    "\n",
    "    tercero = np.where(prediccion == np.amax(prediccion))\n",
    "    pos3 = prediccion[tercero]\n",
    "    texto_prediccion3 = str(diccionario[tercero[0][0]]) + \" \" + str(pos3[0]) + \"%\"\n",
    "\n",
    "    texto_prediccion = texto_prediccion1 + texto_prediccion2 + texto_prediccion3\n",
    "\n",
    "    return texto_prediccion\n",
    "\n",
    "def pintar_test(red): # Esto nos pinta las predicciones\n",
    "    rows = 14\n",
    "    columns =1\n",
    "\n",
    "    image_size = (xpixel, ypixel)\n",
    "\n",
    "    # Generamos el dataset de test (14 imagenes)\n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../Dataset/imgs/imgstest/',\n",
    "        image_size=image_size,\n",
    "        label_mode=None,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    predicciones = red.predict(test_dataset)\n",
    "    predicciones *= 100\n",
    "    \n",
    "    fig = plt.figure(figsize=(55, 55))\n",
    "\n",
    "    for images in test_dataset.take(1):\n",
    "        for i in range(9):\n",
    "            prediccion = predicciones[i]\n",
    "            texto_prediccion = buscar_prediccion(prediccion)\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(texto_prediccion)\n",
    "\n",
    "# Creación del csv. Como son 3.4GB de fotos de test no se adjunta las imágenes en el entregable pero\n",
    "# este es el código que hemos usado para crear el csv\n",
    "\n",
    "def csv_test(red): \n",
    "    image_size = (xpixel, ypixel)\n",
    "    \n",
    "    test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=r'../../imgstest/',\n",
    "        image_size=image_size,\n",
    "        label_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    predicciones = red.predict(test_dataset,verbose = 1)\n",
    "    predicciones = predicciones * 100\n",
    "    \n",
    "    # Añadimos las predicciones\n",
    "    df = pd.DataFrame(predicciones, columns = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9'])\n",
    "    df.to_csv('resultados_test.csv')\n",
    "\n",
    "    # Añadimos el tag de el nombre de la imágen\n",
    "    nombre = os.listdir('../../imgstest/test')\n",
    "    df['imagen'] = nombre\n",
    "\n",
    "pintar_test(model)\n",
    "csv_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio de otras arquitecturas de red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de convolución y pooling\n",
    "\n",
    "A continuación estudiaremos 4 arquitecturas de red sin dropout para ver cuales es la mejor arquitectura respecto a número de capas de convolución y pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de influencia de MLP\n",
    "\n",
    "A continuación estudiaremos 2 arquitecturas de red para ver cuales es la mejor arquitectura respecto al número de neuronas de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_5(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(32/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_6(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(1024/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_7(n_conv1, n_conv2, n_conv3, x, y, lr): # red original con los valores de MLP cambiados\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    # model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(4096/2, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estudio de dropout\n",
    "\n",
    "A continuación estudiaremos las 4 primeras arquitecturas de red con dropout para ver cuales es la influencia del dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_1_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(\n",
    "        3, 3), padding='same', activation='relu'))\n",
    "    # Capas convolucionales\n",
    "    model.add(MaxPooling2D())\n",
    "    #   >este bloque se puede seguir añadiendo, quiza con menos neuronas, o menos capas convolucionales, pongo dos por dar un ejemplo nada mas\n",
    "    # (3,3) es mucho se recomienda unsar 1x1 cuando las img no son mayores de 128x128 la nuestra es 128x96\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Dropout(0.2)) # dropout\n",
    "\n",
    "    model.add(MaxPooling2D())\n",
    "   # model.add(Conv2D(n_conv3,(3,3),activation='relu',padding='same'))                                       #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_2_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_3_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    # capas convolucionales concatenadas\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_D(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DPlus(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.6))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelar_red_4_DExce(n_conv1, n_conv2, n_conv3, x, y, lr):\n",
    "    model = Sequential()\n",
    "    # Capa input\n",
    "    # Como no he rescalado nada usamos esta capa para normalizar los datos\n",
    "    model.add(Rescaling(1./255, input_shape=(x, y, 3)))\n",
    "\n",
    "    model.add(Conv2D(filters=n_conv1, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Conv2D(n_conv2, (3, 3), activation='relu', padding='same'))                                   \n",
    "    model.add(Conv2D(n_conv3, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D()) #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.9))\n",
    "\n",
    "    model.add(Conv2D(n_conv3*2, (3, 3), activation='relu', padding='same'))    # lo multiplico por 4 para que aumente la profundidad                               \n",
    "    model.add(Conv2D(n_conv3*4, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.9))\n",
    "    # Capa fully-connected - MLP\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))  # red fully-connected\n",
    "    model.add(Dense(64, activation='relu'))  # red fully-connected\n",
    "    # capa de salida(softmaxx)\n",
    "    model.add(Dense(num_clases, activation='softmax'))\n",
    "\n",
    "    # Compilamos\n",
    "    # https://keras.io/api/optimizers/adam/ ?\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de diferentes redes\n",
    "\n",
    "Una vez tenemos todas las posibles redes, las entrenamos y evaluamos para ver su output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 1\n",
    "\n",
    "3 capas de convolución con su respectivo pooling cada una. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 2\n",
    "\n",
    "3 capas de convolución seguidas con un único pooling al final. Por último, un MPL de 128 y 64 neuronas respectivamente.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 3\n",
    "\n",
    "3 capas de convolución, con un pooling en la 1º y 3º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4\n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 5\n",
    "\n",
    "2^5 neuronal MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_5(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 6\n",
    "\n",
    "2^10 neuronas MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_6(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 7\n",
    "\n",
    "2^12 neuronas MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_7(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 1 con dropout\n",
    "\n",
    "3 capas de convolución con su respectivo pooling cada una. Por último, un MPL de 128 y 64 neuronas respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_1_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 2 con dropout\n",
    "\n",
    "3 capas de convolución seguidas con un único pooling al final. Por último, un MPL de 128 y 64 neuronas respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_2_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 3 con dropout\n",
    "\n",
    "3 capas de convolución, con un pooling en la 1º y 3º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_3_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4 con droput\n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_D(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4 \n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. \n",
    "Dropout alto (60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DPlus(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red 4\n",
    "\n",
    "5 capas de convolución, con un pooling en la 1º, 3º y 5º capa de convolución. Por último, un MPL de 128 y 64 neuronas respectivamente. \n",
    "Dropout excesivamente alto (90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Modelar_red_4_DExce(16, 32, 64, xpixel, ypixel, l_rate)\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model,train)\n",
    "val_loss, val_acc = Evaluar(model,validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio learning rates\n",
    "\n",
    "Por último estudiamos el learning rate más adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rates = [0.00001,0.0001,0.001,0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficas learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[0])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[0])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[1])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[1])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate de\", l_rates[2])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[2])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate de 0.01\n",
      "Epoch 1/15\n",
      "1122/1122 [==============================] - 36s 31ms/step - loss: 2.8056 - accuracy: 0.1048 - val_loss: 2.3044 - val_accuracy: 0.1039\n",
      "Epoch 2/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3014 - accuracy: 0.1071 - val_loss: 2.3046 - val_accuracy: 0.1026\n",
      "Epoch 3/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1061 - val_loss: 2.3047 - val_accuracy: 0.1039\n",
      "Epoch 4/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1071 - val_loss: 2.3043 - val_accuracy: 0.1039\n",
      "Epoch 5/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1058 - val_loss: 2.3046 - val_accuracy: 0.1026\n",
      "Epoch 6/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3014 - accuracy: 0.1053 - val_loss: 2.3043 - val_accuracy: 0.1026\n",
      "Epoch 7/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1054 - val_loss: 2.3042 - val_accuracy: 0.1039\n",
      "Epoch 8/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3016 - accuracy: 0.1078 - val_loss: 2.3043 - val_accuracy: 0.1026\n",
      "Epoch 9/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3014 - accuracy: 0.1076 - val_loss: 2.3041 - val_accuracy: 0.1088\n",
      "Epoch 10/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3014 - accuracy: 0.1057 - val_loss: 2.3044 - val_accuracy: 0.1039\n",
      "Epoch 11/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1089 - val_loss: 2.3044 - val_accuracy: 0.1026\n",
      "Epoch 12/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3013 - accuracy: 0.1071 - val_loss: 2.3041 - val_accuracy: 0.1039\n",
      "Epoch 13/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3011 - accuracy: 0.1075 - val_loss: 2.3043 - val_accuracy: 0.1088\n",
      "Epoch 14/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3012 - accuracy: 0.1060 - val_loss: 2.3044 - val_accuracy: 0.1026\n",
      "Epoch 15/15\n",
      "1122/1122 [==============================] - 34s 30ms/step - loss: 2.3014 - accuracy: 0.1060 - val_loss: 2.3045 - val_accuracy: 0.1039\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.3026 - accuracy: 0.1048\n",
      "281/281 [==============================] - 3s 12ms/step - loss: 2.3045 - accuracy: 0.1039\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rate de\", l_rates[3])\n",
    "model = Modelar_red(16, 32, 64, xpixel, ypixel, l_rates[3])\n",
    "# si no satisface al callback esto parara\n",
    "history = Entrenar(model, train, validation, epoch, batch)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "train_loss, train_acc = Evaluar(model, train)\n",
    "val_loss, val_acc = Evaluar(model, validation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae011bd53887c3dab0246b667a51a9d69f3606ce064e4af6110c47693a17202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow-environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
